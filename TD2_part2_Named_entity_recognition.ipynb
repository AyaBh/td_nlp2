{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b7c0b40",
   "metadata": {},
   "source": [
    "# TD2 part 2: Named entity recognition\n",
    "\n",
    "Dans ce TD, nous allons prendre un datasets où les noms de personnes sont taggés.<br>\n",
    "Nous allons transformer ces données en tenseurs X, y et attention_mask.<br>\n",
    "Nous allons créer un modèle RNN pour prédire si un mot est un nom de personne.<br>\n",
    "Nous allons ensuite créer la loop avec l'optimizer pour apprendre le modèle.<br>\n",
    "Du modèle appris (prédisant sur les tokens), nous allons postprocess les prédictions pour avoir les prédictions sur les noms.\n",
    "\n",
    "Un fois que la loop est créée et que le modèle apprend, nous allons changer la structure du modèle:\n",
    "- Changer learning rate. Comment se comporte le modèle\n",
    "- Ajouter des couches denses, ReLU, dropout, normalization\n",
    "- Changer le nombre de layers du RNN, LSTM.\n",
    "\n",
    "Lorsqu'on a un bon modèle de prédiction pour les noms de personnes, nous allons l'appliquer à notre projet fil rouge.\n",
    "Utilisez-le tel que. Quelle accuracy ?\n",
    "Ré-entrainez la (les) dernière(s) couche(s) du modèle sur notre jeu de données. A-t-il gagné en accuracy ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.9 MB 3.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers<0.19,>=0.14\n",
      "  Downloading tokenizers-0.15.0-cp39-cp39-macosx_10_7_x86_64.whl (2.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.6 MB 38.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0.1-cp39-cp39-macosx_10_9_x86_64.whl (197 kB)\n",
      "\u001b[K     |████████████████████████████████| 197 kB 55.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /Users/stockly/Library/Python/3.9/lib/python/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/stockly/Library/Python/3.9/lib/python/site-packages (from transformers) (1.26.2)\n",
      "Collecting requests\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "\u001b[K     |████████████████████████████████| 62 kB 841 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /Users/stockly/Library/Python/3.9/lib/python/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: filelock in /Users/stockly/Library/Python/3.9/lib/python/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/stockly/Library/Python/3.9/lib/python/site-packages (from transformers) (4.66.1)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "\u001b[K     |████████████████████████████████| 311 kB 26.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.4.0-cp39-cp39-macosx_10_7_x86_64.whl (439 kB)\n",
      "\u001b[K     |████████████████████████████████| 439 kB 30.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: fsspec>=2023.5.0 in /Users/stockly/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/stockly/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 448 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.3.2-cp39-cp39-macosx_10_9_x86_64.whl (122 kB)\n",
      "\u001b[K     |████████████████████████████████| 122 kB 12.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "  Downloading certifi-2023.11.17-py3-none-any.whl (162 kB)\n",
      "\u001b[K     |████████████████████████████████| 162 kB 54.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.1.0-py3-none-any.whl (104 kB)\n",
      "\u001b[K     |████████████████████████████████| 104 kB 59.0 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: urllib3, idna, charset-normalizer, certifi, requests, pyyaml, huggingface-hub, tokenizers, safetensors, transformers\n",
      "Successfully installed certifi-2023.11.17 charset-normalizer-3.3.2 huggingface-hub-0.19.4 idna-3.4 pyyaml-6.0.1 requests-2.31.0 safetensors-0.4.0 tokenizers-0.15.0 transformers-4.35.2 urllib3-2.1.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86402ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stockly/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/stockly/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f552fb",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Télécharger le dataset MultiNERD FR [ici](https://github.com/Babelscape/multinerd)<br>\n",
    "Mettez les données dans le dossier data/raw du projet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "157bc913",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_multinerd_person_words(filename=\"./data/raw/train_fr.tsv\"):\n",
    "    with open(filename) as f:\n",
    "        tagged_words = [line.strip().split(\"\\t\") for line in f]\n",
    "        \n",
    "        # Joining words until we meet a dot\n",
    "        # Word's label is 1 if 'PER' is in its tag\n",
    "        sentences = []\n",
    "        sentence_labels = []\n",
    "        this_word = []\n",
    "        this_labels = []\n",
    "        for tagged_word in tagged_words:\n",
    "            if len(tagged_word) < 3:\n",
    "                # not a tagged word\n",
    "                continue\n",
    "            word = tagged_word[1]\n",
    "            tag = tagged_word[2]\n",
    "        \n",
    "            if word == '.':\n",
    "                sentences.append(\" \".join(this_word))\n",
    "                sentence_labels.append(this_labels)\n",
    "            \n",
    "                this_word = []\n",
    "                this_labels = []\n",
    "            else:\n",
    "                this_word.append(word)\n",
    "                this_labels.append(1 * tag.endswith(\"PER\"))\n",
    "\n",
    "    return sentences, sentence_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcba104b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, labels = extract_multinerd_person_words(\"./data/raw/train_fr.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9b09cc",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "En utilisant le tokenizer d'HuggingFace \"camembert-base\":\n",
    "- Transformer les phrases en tokens\n",
    "- Obtenez des vecteur y qui ont le même nombre d'entrées qu'il y a de tokens dans la phrase\n",
    "- Ayez un tenseur \"attention_mask\" pour savoir sur quels tokens on cherche à predire le label\n",
    "- Transformez les tokens en token_ids (avec le tokenizer)\n",
    "Avec tout cela, vous pouvez former vos tenseurs X, Y et attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c8ff1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"camembert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04a3802c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tokens_and_labels_and_attention_mask(tokenizer, sentence, labels):\n",
    "    words = sentence.split()\n",
    "    tokens = []\n",
    "    tokens_label = []\n",
    "    attention_mask = []\n",
    "    \n",
    "    for word, label in zip(words, labels):\n",
    "        this_tokens = tokenizer.tokenize(word)\n",
    "        tokens += this_tokens\n",
    "        \n",
    "        this_labels = [0] * len(this_tokens)\n",
    "        this_labels[0] = label        \n",
    "        tokens_label += this_labels\n",
    "        \n",
    "        this_attention_mask = [1] + [0] * (len(this_tokens) - 1)\n",
    "        attention_mask += this_attention_mask\n",
    "        \n",
    "    return tokens, tokens_label, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "066d1710",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, label, padding_masks = build_tokens_and_labels_and_attention_mask(tokenizer, sentences[0], labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-05 16:14:28.930325: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: tensor([   5, 1285,   16,  164,    6])\n",
      "Tokens Label: tensor([1, 0, 1, 0])\n",
      "Attention Mask: tensor([1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"camembert-base\")\n",
    "\n",
    "def build_tokens_and_labels_and_attention_mask(tokenizer, sentence, labels):\n",
    "    # Tokenize the sentence\n",
    "    tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(sentence)))\n",
    "    \n",
    "    # Initialize labels and attention mask lists\n",
    "    tokens_label = []\n",
    "    attention_mask = []\n",
    "    \n",
    "    # Process each word and its corresponding label\n",
    "    for word, label in zip(tokens, labels):\n",
    "        # Tokenize the word\n",
    "        this_tokens = tokenizer.tokenize(word)\n",
    "        tokens_label += [label] * len(this_tokens)\n",
    "        \n",
    "        # Create attention mask\n",
    "        this_attention_mask = [1] + [0] * (len(this_tokens) - 1)\n",
    "        attention_mask += this_attention_mask\n",
    "        \n",
    "    # Encode the tokens to obtain token IDs\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    \n",
    "    return token_ids, tokens_label, attention_mask\n",
    "\n",
    "# Example usage\n",
    "sentence = \"Bonjour le monde\"\n",
    "labels = [1, 0, 1, 0]\n",
    "token_ids, tokens_label, attention_mask = build_tokens_and_labels_and_attention_mask(tokenizer, sentence, labels)\n",
    "\n",
    "# Convert lists to tensors\n",
    "token_ids = torch.tensor(token_ids)\n",
    "tokens_label = torch.tensor(tokens_label)\n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "print(\"Token IDs:\", token_ids)\n",
    "print(\"Tokens Label:\", tokens_label)\n",
    "print(\"Attention Mask:\", attention_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3432, -1.2361]], grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=50, hidden_dim=20, tagset_size=2):\n",
    "        super(RNNModel, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.linear = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.embedding(input_ids)\n",
    "\n",
    "        rnn_out, _ = self.rnn(embedded)\n",
    "\n",
    "        rnn_out = rnn_out[:, -1, :]\n",
    "\n",
    "        relu_out = self.relu(rnn_out)\n",
    "\n",
    "        linear_out = self.linear(relu_out)\n",
    "\n",
    "        output = self.softmax(linear_out)\n",
    "\n",
    "        return output\n",
    "\n",
    "vocab_size = 52000  \n",
    "model = RNNModel(vocab_size)\n",
    "\n",
    "input_ids = torch.tensor([[1, 2, 3, 4, 5]])  \n",
    "\n",
    "output = model(input_ids)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb94a39",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Contruisez un modèle RNN comme dans la partie 1. Pour l'instant, il prendra comme arguments:\n",
    "- Vocab size: le nombre de différents tokens du tokenizer (52 000 pour camembert-base)\n",
    "- Embedding dim: la dimension de l'embedding des tokens (par défaut 50)\n",
    "- hidden_dim: la dimension de l'état récurrent de votre RNN (par défaut 20)\n",
    "- tagset_size: la nombre de classes possibles pour les prédictions (ici 2)\n",
    "\n",
    "Dans le forward, votre modèle enchaînera les couches suivantes:\n",
    "- Un embedding\n",
    "- Un RNN\n",
    "- Un ReLU\n",
    "- Une couche linéaire\n",
    "- Un softmax pour que la somme des prédictions pour une entrée soit égale à 1 (la prédiction pour un élément et sa probabilité d'être dans chaque classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86e661ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=50, hidden_dim=20, tagset_size=2):\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        # Embedding Layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        \n",
    "        # RNN Layer\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        # ReLU Activation\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Linear Layer\n",
    "        self.linear = nn.Linear(hidden_dim, tagset_size)\n",
    "        \n",
    "        # Softmax Activation\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # Embedding\n",
    "        embedded = self.embedding(input_ids)\n",
    "        \n",
    "        # RNN\n",
    "        rnn_out, _ = self.rnn(embedded)\n",
    "        \n",
    "        # Select the output of the last time step\n",
    "        rnn_out = rnn_out[:, -1, :]\n",
    "        \n",
    "        # ReLU activation\n",
    "        relu_out = self.relu(rnn_out)\n",
    "        \n",
    "        # Linear layer\n",
    "        linear_out = self.linear(relu_out)\n",
    "        \n",
    "        # Softmax activation\n",
    "        output = self.softmax(linear_out)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d04004a",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "Je fournis ici une fonction prenant un modèle, des tenseurs X, y, attention_mask.\n",
    "Pour chaque batch:\n",
    "- La loop utilise le modèle pour prédire sur x_batch\n",
    "- Avec attention_mask, elle identifie sur quels tokens les prédictions compte\n",
    "- Elle regarde la cross entropy entre y\\[attention_ids\\] et yhat\\[attention_ids\\]\n",
    "- Elle output un dictionnaire avec le model et la loss au fur et à mesure des itérations\n",
    "\n",
    "Entraînez le modèle avec vos données. <br>\n",
    "Plottez la loss history.<br>\n",
    "Itérez sur le modèle pour l'améliorer:\n",
    "- Changer learning rate. Comment se comporte le modèle\n",
    "- Ajouter des couches denses, ReLU, dropout, normalization\n",
    "- Changer le nombre de layers du RNN, LSTM.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58b9833f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X, y, attention_masks, n_epochs=100, lr=0.05, batch_size=128):\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    loss_history = []\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(X, y, attention_masks)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for i, (x_batch, y_batch, mask) in enumerate(loader):\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            ids = mask.reshape(-1)\n",
    "            yhat = model(x_batch).reshape((-1, 2))[ids]\n",
    "            this_y = y_batch.reshape(-1)[ids]\n",
    "            \n",
    "            loss = loss_function(yhat, this_y)\n",
    "            loss.backward()\n",
    "            \n",
    "            loss_history.append(loss.clone().detach())\n",
    "        \n",
    "            optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Got loss at {epoch}\", np.mean(loss_history[-10:]))\n",
    "    \n",
    "    return {\"model\": model, \"loss_history\": loss_history}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cf63f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train_model(model, X, y, attention_mask, learning_rate=0.001, num_epochs=10):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    loss_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for x_batch, y_batch, mask_batch in zip(X, y, attention_mask):\n",
    "            model.train()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(x_batch)\n",
    "\n",
    "            attention_ids = torch.nonzero(mask_batch).view(-1)\n",
    "\n",
    "            loss = criterion(outputs[attention_ids], y_batch[attention_ids])\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        epoch_loss /= len(X)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
    "        loss_history.append(epoch_loss)\n",
    "\n",
    "    print('Entraînement terminé!')\n",
    "    return model, loss_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21a6933",
   "metadata": {},
   "source": [
    "## Postprocessing\n",
    "\n",
    "Créer une fonction prenant les prédictions du modèle (au niveau token) et sort les prédictions au niveau mot.<br>\n",
    "Par exemple, admettons que, pour un mot, la prédiction du 1er token est la seule qu'on considère.<br>\n",
    "si la phrase est \"Bonjour John\", avec les tokens \\[\"bon\", \"jour\", \"Jo\", \"hn\"\\] avec les predictions \\[0.12, 0.65, 0.88, 0.45\\]<br>\n",
    "Je veux récupérer les prédictions \"bonjour\": 0.12, \"John\": 0.88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a0316b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bonjour': 0.12, 'John': 0.88}\n"
     ]
    }
   ],
   "source": [
    "def postprocess_predictions(token_predictions, tokens, word_indices):\n",
    "    word_predictions = {}\n",
    "\n",
    "    for word, start_idx, end_idx in word_indices:\n",
    "        word_token_preds = token_predictions[start_idx:end_idx + 1]\n",
    "        \n",
    "        word_prediction = word_token_preds[0]\n",
    "\n",
    "        word_predictions[word] = word_prediction\n",
    "\n",
    "    return word_predictions\n",
    "\n",
    "token_predictions = [0.12, 0.65, 0.88, 0.45]\n",
    "tokens = [\"bon\", \"jour\", \"Jo\", \"hn\"]\n",
    "word_indices = [(\"bonjour\", 0, 1), (\"John\", 2, 3)]\n",
    "\n",
    "result = postprocess_predictions(token_predictions, tokens, word_indices)\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
